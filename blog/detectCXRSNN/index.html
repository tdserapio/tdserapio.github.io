<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Detecting Change Between CXR Images Using Siamese Neural Networks</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-interactiveBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="eab740b4-098b-4858-be4a-f6296e5692ae" class="page sans"><header><h1 class="page-title">Detecting Change Between CXR Images Using Siamese Neural Networks</h1><p class="page-description"></p></header><div class="page-body"><p id="49d3ba13-c20d-4afe-a2ee-eb462434e5dc" class="">Siamese neural networks, also called twin neural networks, are really helpful in most machine learning tasks that include the comparison of two objects. This architecture is commonly used in image comparison tasks such as facial recognition, and temporal characterizations. </p><p id="1c89d18c-de17-4d01-a568-7f9b1827ccce" class="">With siamese networks, it becomes possible to detect changes between a series of patient’s chest X-rays (CXRs). For this article, we will focus on the progression of pleural effusion among patients as an example. We will use the ImaGenome dataset, which is a collection of temporal series of CXR images with descriptions.</p><h1 id="a4e5ec97-b7ef-4b27-8bb7-08909995c9d4" class="">What are Siamese neural networks, exactly?</h1><p id="a6dd7cb4-cd82-4663-bd85-aa3689af36cf" class="">Siamese neural networks are two parallel and identically-weighted neural networks that take in two (different) input vectors per network. In our use case, these input vectors two CXR images. </p><div id="7893fbad-516b-4272-9b7e-d97e005b0d03" class="column-list"><div id="acccf98a-b442-4908-8f03-1f789e9297d5" style="width:50%" class="column"><figure id="f64d18da-e9c6-49d4-96e5-18493c284261" class="image"><a href="Screen_Shot_2023-07-23_at_8.23.28_PM.png"><img style="width:349px" src="Screen_Shot_2023-07-23_at_8.23.28_PM.png"/></a><figcaption>Taken from ImaGenome dataset</figcaption></figure></div><div id="11520144-4d58-42d0-a7b4-780648306834" style="width:50%" class="column"><figure id="c8514b37-598d-4e32-a4e4-bd06a456392b" class="image"><a href="Screen_Shot_2023-07-23_at_8.23.33_PM.png"><img style="width:348px" src="Screen_Shot_2023-07-23_at_8.23.33_PM.png"/></a><figcaption>Taken from ImaGenome dataset</figcaption></figure></div></div><p id="29d671df-c70f-4e84-ba90-ed0f2f4309af" class="">These input vectors are then fed into their respective convolutional neural networks, and the neural networks generate two separate embeddings (vectors that are far easier to compare to each other). This could be thought of as the model outputting its own version of the original input.</p><figure id="9644cc91-7644-4263-b45b-c428cc9cca2a" class="image"><a href="Screen_Shot_2023-07-24_at_8.30.41_PM.png"><img style="width:712px" src="Screen_Shot_2023-07-24_at_8.30.41_PM.png"/></a><figcaption>Figure 1</figcaption></figure><p id="c949834a-940b-4afa-8418-45bbd900e359" class="">The special part about siamese networks is the part where these output vectors are compared against each other to come up with a prediction (eg. whether or not a change was detected). </p><p id="d91f653d-444d-4172-be6c-f7f07af82485" class="">This prediction is then trained on a loss function (cross-entropy loss). The reversed arrows emphasize the data retraining itself by going back from the loss function until the first layer of the CNN. This could be thought of as the model learning how to make its own version of the original input. </p><figure id="8da97a79-7994-44e0-a615-8200fd0cc5b9" class="image"><a href="Screen_Shot_2023-07-24_at_8.30.49_PM.png"><img style="width:712px" src="Screen_Shot_2023-07-24_at_8.30.49_PM.png"/></a><figcaption>Figure 2</figcaption></figure><p id="be544d43-853e-4f4f-a4b7-9ddf722c44b1" class="">This mechanism allows for the model to form a prediction based on two (or more) separate inputs. In fact, by simply adding more parallel and identically-weighted neural networks, we could take in even more inputs. However, this also gets computationally-heavy, so models currently in use tend to use networks that take in two inputs. Now that we understand the essence of Siamese neural networks, we can finally implement them!</p><h1 id="1261d9ba-1827-4afa-8efb-a45afa2ebfca" class="">Implementing Siamese neural networks</h1><h2 id="2488719d-05f5-498d-bd46-3b0641e59126" class="">Setting-Up The Environment</h2><p id="417c5499-6c11-4f26-b66d-f9033dd4c79c" class="">I used PyTorch for the implementation of Siamese Neural Networks as PyTorch has models specifically trained on X-ray images (PyTorchXRayVision). PyTorch is also very easy to use. I also used <a href="http://colab.research.google.com"><code>colab.research.google.com</code></a> as the environment, but any Jupyter-like software would be fine. </p><ul id="4a18ce42-1367-4638-931f-47e23f2ba476" class="toggle"><li><details open=""><summary>Install and import all libraries needed</summary><pre id="4a176c5b-30f9-400b-bbcc-46147444bf6b" class="code"><code># Installations
!pip install transformers
!pip install torchmetrics
!pip install torchxrayvision
!pip install pytorch-lightning
!pip install scikit-plot</code></pre><pre id="8cde25df-cecd-47e7-969f-55c0347d472e" class="code"><code># File Management
from google.colab import drive
import pandas as pd
import matplotlib.pyplot as plt

# Utilities
from __future__ import print_function
import argparse, random, copy
import numpy as np
import tqdm

# Torch imports
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

import pytorch_lightning as pl

import torchvision
from torchvision import datasets
from torchvision import transforms as T
from torch.optim.lr_scheduler import StepLR

import torchxrayvision as xrv

# SKLearn
import sklearn
import scikitplot as skplt
from sklearn.model_selection import train_test_split

# Premade Transformers
from transformers import AutoModel</code></pre><p id="4ba999c6-40cd-4596-bf6a-43b9cf513e56" class="">
</p></details></li></ul><h2 id="a9fea78d-c7d7-4f7c-b53b-b5089b8e6a6e" class="">Preprocessing The Images</h2><p id="0e8d66e9-1cd2-4ac0-9e46-b2809a0f48a7" class="">After setting-up PyTorch, we then need to have data to train the model on. In this case, I will use the ImaGenome dataset. Here is the link to the Google Drive of the <a href="https://drive.google.com/drive/folders/1waWP_DagIdxgFKBz0QIObNgVt9tF5-WB?usp=sharing">preprocessed dataset</a>. From here, all we need to do is read the files in the dataset. We would convert these files into <code>pandas.DataFrames</code> to make it very convenient to access the data from these files. </p><ul id="99d19d9f-341e-4932-a5fc-df360c447181" class="toggle"><li><details open=""><summary>Read files in ImaGenome folder as DataFrames</summary><pre id="3b97018b-a8b2-4ea1-9de2-922ee503acb5" class="code"><code># Note: subject_id -&gt; current, object_id -&gt; previous
csv_df = pd.read_csv(&quot;./chest_imagenome_pleural_effusion.csv&quot;)
prevpkl = pd.read_pickle(&quot;./prev_img.pickle&quot;)
currentpkl = pd.read_pickle(&quot;./curr_img.pickle&quot;)</code></pre></details></li></ul><p id="b0babd3f-924b-45ec-ae92-28a9f9c0b38f" class="">In addition, it would be far more convenient to store these DataFrames as a <code>Dataset</code> object, which is something we would have to implement by ourselves. Note that for this task, we would need to turn the images from the ImaGenome dataset into grayscale as the model that we will use can only process grayscale images. </p><ul id="fd23b3fb-534c-4ea4-bf6a-ac372f005ce3" class="toggle"><li><details open=""><summary>SiameseDataset object</summary><pre id="b3de8397-8be0-4c50-8c26-744694b548da" class="code"><code>class SiameseDataset(Dataset):
    def __init__(self, data, prev_pkl, current_pkl):
        &quot;&quot;&quot;
        The SiameseDataset object stores the data into something neat and accessible.
        Also, makes the images have 3-&gt;1 channel (grayscale).
        &quot;&quot;&quot;
        super(SiameseDataset, self).__init__()
        self.data = data
        self.prev_pkl = prev_pkl
        self.current_pkl = current_pkl
        self.greyscale = torchvision.transforms.Grayscale(1)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
				&quot;&quot;&quot;
				This returns a dict() object that has three items:
				1. The first CXR
				2. The proceeding CXR
				3. Whether or not a change occured from 1.) -&gt; 2.)
				&quot;&quot;&quot;
        image_1 = self.prev_pkl[self.data[&quot;object_id&quot;][index]]
        image_1 = self.greyscale(image_1)
        image_2 = self.current_pkl[self.data[&quot;subject_id&quot;][index]]
        image_2 = self.greyscale(image_2)
        if self.data[&quot;comparison&quot;][index] == &quot;no change&quot;:
          target = 0
        else:
          target = 1

        return {&quot;prev_img&quot;: image_1, &quot;curr_img&quot;: image_2, &quot;change&quot;: target}</code></pre></details></li></ul><h2 id="fff57463-163a-4fb7-9522-4a1e0c271534" class="">Siamese neural network implementation</h2><h3 id="6f5908f5-3ade-46d3-bc7c-a93bd1cbfc91" class="">Forward Propagation</h3><p id="61992be8-09dd-42d2-b89c-5080a610b512" class="">First, the model takes in two images (A and B), which will go through two identical and parallel convolutional neural networks (CNN). The two outputs of the CNN are then combined, thus generating the model’s prediction. This is called forward propagation, wherein the data is passed on to the next layer of the neural network. </p><figure id="7b92a483-8736-416b-8243-70fa13e6cf8d" class="image"><a href="Screen_Shot_2023-07-24_at_8.31.01_PM.png"><img style="width:710px" src="Screen_Shot_2023-07-24_at_8.31.01_PM.png"/></a><figcaption>Figure 3</figcaption></figure><p id="b497c2ec-c31d-4631-9850-8d7cea7a23b3" class="">To actually implement this in Python is not too difficult. We would store this architecture in a Python class called <code>SiameseNetwork</code>. </p><ul id="bf9391fe-c7b2-440d-a5da-eba09184cf70" class="toggle"><li><details open=""><summary>Complete code</summary><pre id="ce07d7c9-79cf-4b9e-a351-8ce82bcce19b" class="code"><code># Configurations for Siamese Network
config = {
    &quot;d_model&quot;: 512,
    &quot;dropout&quot;: 0.2,
    &quot;num_classes&quot;: 2,
    &quot;lr&quot;: 1e-3
}

# Siamese Network Architecture
class SiameseNetwork(nn.Module):
    def __init__(self):
        &quot;&quot;&quot;
        The siamese network architecture makes use of two parallel neural
        networks that learn to, in this case, be able to successfully
        encode and distinguish between two images.
        &quot;&quot;&quot;
        super(SiameseNetwork, self).__init__()

        # 101-elastic is trained on PadChest, NIH, CheXpert, and MIMIC datasets
        self.cnn1 = xrv.autoencoders.ResNetAE(weights=&quot;101-elastic&quot;)
        self.nclasses = config[&quot;num_classes&quot;]

        outdim = 512 * 3 * 3 * 2

        for param in self.cnn1.parameters():
            param.requires_grad = False

        self.fc = nn.Linear(outdim, config[&quot;d_model&quot;])
        self.dropout = nn.Dropout(config[&quot;dropout&quot;], inplace=False)
        self.fc_final = nn.Linear(config[&quot;d_model&quot;], config[&quot;num_classes&quot;])
        self.learning_rate = config[&quot;lr&quot;]

    def forward_once(self, x):
				&quot;&quot;&quot;
				This is the forward propagation through the CNN
				&quot;&quot;&quot;
        output = self.cnn1(x)
        output = output[&quot;z&quot;].view(-1, 512*3*3)
        return output

    def forward(self, input1, input2):
				&quot;&quot;&quot;
				This is the forward propagation throughout the model
				&quot;&quot;&quot;
        # Parallel networks in action
        prev_image_features = self.forward_once(input1)
        curr_image_features = self.forward_once(input2)

        # Combine the features, and generate the output
        image_features = torch.cat((prev_image_features, curr_image_features), 1)
        image_features = F.relu(self.fc(image_features))
        image_features = self.dropout(image_features)
        classifier_output = self.fc_final(image_features)

        return classifier_output</code></pre></details></li></ul><h3 id="0caafea7-b22c-4bd5-b7b1-4eeca2de6a92" class="">Backward Propagation</h3><p id="2fbeea0b-8842-4903-a350-870d0acacfb7" class="">From there, the output is passed through a loss function, which in turn retrains the model. The loss function calculates how well the model did by comparing the generated output to the “correct” output. This is called backward propagation, wherein the model traces back its steps to see where it went wrong. This leads us to training and testing the model. </p><figure id="0e2fde59-9517-4a67-b503-6ae6be7af64a" class="image"><a href="Screen_Shot_2023-07-24_at_8.31.10_PM.png"><img style="width:712px" src="Screen_Shot_2023-07-24_at_8.31.10_PM.png"/></a><figcaption>Figure 4</figcaption></figure><h2 id="1b1ac00e-28fe-492c-a530-cf3a76dcd929" class="">Training and Testing The Model</h2><p id="7f419e5d-311e-46ac-a250-c4d16db3463d" class="">Training would be giving the model a multitude of CXR images, and then performing the forward and backward propagation processes repeatedly until it learns to output correct predictions. It is able to learn to do so with an “optimizer” (which follows the loss function) tweaking and readjusting the neural network’s weights until it is able to generate a correct prediction for most samples. You can imagine this as a teacher assisting a student based on their score on a test. </p><ul id="d93b8d3a-8f68-42a8-adbd-3c28d169f217" class="toggle"><li><details open=""><summary>Training the model</summary><pre id="d4482ccb-d719-45ee-b205-813daf1db3a1" class="code"><code>def train(model, device, train_loader, optimizer, epoch):
    &quot;&quot;&quot;
    This uses CrossEntropyLoss. Although BinaryEntropyLoss function could
    also be used, the documentation preferred using CrossEntropyLoss.

    Other than that, this is essentially training the model.
    &quot;&quot;&quot;
    model.train()
    criterion = nn.CrossEntropyLoss()

    for batch_idx, batch in enumerate(tqdm.tqdm(train_loader)):
				# &quot;Loads in&quot; the images into the model
        targets = batch[&quot;change&quot;].type(torch.LongTensor)
        images_1 = batch[&quot;prev_img&quot;].to(device)
        images_2 = batch[&quot;curr_img&quot;].to(device)
        targets = batch[&quot;change&quot;].to(device)

        optimizer.zero_grad()
        outputs = model(images_1, images_2) # Forward Propagation
        loss = criterion(outputs, targets) # Loss Function

        loss.backward() # Backwards Propagation
        optimizer.step() # Optimizer tweaking the model

        if batch_idx % 100 == 0:
            print(&#x27;Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}&#x27;.format(
                epoch, batch_idx * len(images_1), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))</code></pre></details></li></ul><figure id="e96cb384-3b49-4896-8ac1-9e94d7874af5" class="image"><a href="Screen_Shot_2023-07-24_at_8.31.20_PM.png"><img style="width:708px" src="Screen_Shot_2023-07-24_at_8.31.20_PM.png"/></a><figcaption>Figure 5</figcaption></figure><p id="f6875cdc-d359-4914-ab06-f2d3c144d9cb" class="">Testing would be giving the model a multitude of CXR images, then seeing how close the model is to correctly predicting the right answer. You can imagine this as a teacher giving a quiz to a student, and seeing how well the student performs. </p><ul id="857e0734-b470-4d19-aa4a-29e7130c86a3" class="toggle"><li><details open=""><summary>Testing the model</summary><pre id="23f522d4-ccad-4ea1-8ced-8ccf443b5732" class="code"><code>def test(model, device, test_loader):
    &quot;&quot;&quot;
    This function tells the accuracy and loss of the model.
    &quot;&quot;&quot;
    model.eval()
    test_loss = 0
    correct = 0

    criterion = nn.CrossEntropyLoss()

    with torch.no_grad():
        for batch in test_loader:
						# &quot;Load in&quot; the images
            targets = batch[&quot;change&quot;].type(torch.LongTensor)
            images_1 = batch[&quot;prev_img&quot;].to(device)
            images_2 = batch[&quot;curr_img&quot;].to(device)
            targets = batch[&quot;change&quot;].to(device)

						# Get the output
            outputs = model(images_1, images_2)
						# Use the loss function as a metric of performance
            test_loss += criterion(outputs, targets).sum().item()
						# Use accuracy as a metric of performance
            pred = outputs.argmax(1)
            correct += pred.eq(targets).sum().item()

    test_loss /= len(test_loader.dataset)

    print(&#x27;\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n&#x27;.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))</code></pre></details></li></ul><figure id="efbba225-d9b7-4e07-978e-51d7147e2e04" class="image"><a href="Screen_Shot_2023-07-24_at_8.31.27_PM.png"><img style="width:710px" src="Screen_Shot_2023-07-24_at_8.31.27_PM.png"/></a><figcaption>Figure 6</figcaption></figure><h2 id="edebaa8b-3f60-4377-899f-6253a113e78a" class="">Output Discussion</h2><p id="6d2fa1eb-3ab8-4e1b-9d3d-2be463d2a935" class="">Here we may see that the Siamese neural networks seemed to overgeneralize into always placing itself on the “no change” category. </p><figure id="8d02a6e9-fcb1-459e-be1a-bfe4304eeeca" class="image"><a href="Screen_Shot_2023-07-23_at_10.37.42_PM.png"><img style="width:672px" src="Screen_Shot_2023-07-23_at_10.37.42_PM.png"/></a></figure><div id="49a6a680-9518-4cad-ab8b-beafa29ebccc" class="column-list"><div id="1c52ee60-c41e-4ce5-b2e7-28a6c8a1419d" style="width:50%" class="column"><figure id="7a303ec8-2d6b-4ac9-b37d-2eaae30a5757" class="image"><a href="Screen_Shot_2023-07-21_at_6.52.37_PM.png"><img style="width:576px" src="Screen_Shot_2023-07-21_at_6.52.37_PM.png"/></a></figure></div><div id="e447d58a-9067-4566-9f56-2a0fecdc712e" style="width:50%" class="column"><figure id="6c113ae3-9e7f-4716-aeed-0fc748b26caf" class="image"><a href="Screen_Shot_2023-07-21_at_6.53.46_PM.png"><img style="width:576px" src="Screen_Shot_2023-07-21_at_6.53.46_PM.png"/></a></figure></div></div><p id="2b4c8a6e-cd4b-4d7f-a8a6-d976eb8da2cb" class="">However, upon further investigation, this is somewhat a data problem which could be solved by adding more data. This is what I discovered with the MNIST dataset, and seeing whether or not the model would be able to detect whether the two handwritten digits are both 1s or not. </p><figure id="3f9d8e64-2d7d-4c29-8d67-c1343ccf484f" class="image"><a href="Screen_Shot_2023-07-23_at_10.06.18_PM.png"><img style="width:1112px" src="Screen_Shot_2023-07-23_at_10.06.18_PM.png"/></a></figure><div id="18335d1e-8783-44d0-9fe7-e41a8ba27f1c" class="column-list"><div id="2635426e-3f1e-4a89-88c5-8940a16f8ce8" style="width:50%" class="column"><figure id="40cf56a4-c0da-4236-8332-39b6ac7c0abe" class="image"><a href="Screen_Shot_2023-07-23_at_10.06.58_PM.png"><img style="width:845px" src="Screen_Shot_2023-07-23_at_10.06.58_PM.png"/></a></figure></div><div id="3a4a0f9c-eb5d-4706-8246-0c353863fcf2" style="width:50%" class="column"><figure id="00a02d6d-9d7b-46a2-b8c8-1b56ecdf54f2" class="image"><a href="Screen_Shot_2023-07-23_at_10.07.06_PM.png"><img style="width:842px" src="Screen_Shot_2023-07-23_at_10.07.06_PM.png"/></a></figure></div></div><p id="3d57be6a-94dc-42ca-b880-633dddf76c1b" class="">Another problem with CXR images and their description is its complexity with the many different organs that may influence the result, thus making it a much harder task. In addition to that, radiologists may sometimes disagree with their results, with one saying “nothing significant changed” while another saying otherwise. However, with the MNIST dataset, we can tell just how powerful Siamese neural networks can be. </p><h1 id="c5a8ca9f-b1a7-4128-a4cb-530f2d182a88" class="">Last Words</h1><p id="3061877f-4512-43bd-98ca-840e80d830b2" class="">In conclusion, siamese neural networks offer a promising approach to detecting changes and tracking progress in medical imaging, especially in chest X-rays. </p><p id="7df4b85c-9d77-47fd-8771-f716d36ac27d" class="">This powerful technology can help medical professionals identify and intervene in conditions such as pleural effusion and other pathologies, perhaps even before they become symptomatic. </p><p id="daca5222-16bc-48e2-b33f-d8c31571df94" class="">The future of medical imaging with artificial intelligence is bright, and we are only scratching the surface of what is possible. </p><p id="63d1acad-e09a-48ca-b0f6-5b61f9de7207" class="">Thank you very much for reading 😊!</p><h1 id="67fb9f5a-9b8b-4927-8ebd-5d5413288830" class="">Resources and Links</h1><ol type="1" id="e3c5da11-d5cf-4fab-a626-00094b6d10a6" class="numbered-list" start="1"><li><a href="https://doi.org/10.13026/wv01-y230">ImaGenome Dataset</a></li></ol><ol type="1" id="7b9ea8f8-fb90-4069-821e-441252858150" class="numbered-list" start="2"><li><a href="https://yann.lecun.com/exdb/mnist/">MNIST Dataset</a></li></ol><ol type="1" id="08bbcf7a-f492-4364-9c58-4e8fc0b4d5a5" class="numbered-list" start="3"><li><a href="https://github.com/tdserapio/CXR-Siamese-Networks">GitHub Repository on Siamese Neural Networks</a></li></ol><ol type="1" id="dd066480-860f-4ed8-a499-1b66812893fd" class="numbered-list" start="4"><li><a href="https://www.youtube.com/watch?v=BcF6FfZHDqA">ADL4CV:DV Lecture on Siamese Networks</a></li></ol></div></article></body></html>